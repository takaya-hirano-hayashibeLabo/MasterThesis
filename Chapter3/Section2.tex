\section{結果} \label{sec:result1}
入力スパイクのタイムスケーリングに対して, 本手法を用いることで, SNNの内部状態をタイムスケーリングに近似可能であることを実験的に示す.
本節では, 入力速度変化に対する内部状態変化の結果を定量的, 定性的に示す.

\subsection{入力速度変化に対する内部状態変化の定量評価}
各ネットワーク構造 (\tabref{tab:model:parameter:linear} - \tabref{tab:model:parameter:resnet}) における入力速度変化に対する内部状態変化の結果を\figref{fig:result1:1:linear} - \figref{fig:result1:1:resnet} に示す.
横軸$a$は入力スパイクのタイムスケール倍率, 縦軸は$a=1.0$のときの内部状態と各タイムスケール倍率$a$に置ける内部状態とのMSEを表す.
ここで, タイムスケール$a$は値が小さいほど入力スパイクの速度が速く, 値が大きいほどその速度が遅いことを表す.
また, 青色のグラフが通常のSNN, 黄緑色のグラフが本手法を用いたSNNを表す.
さらに, 時定数の初期値として$\tau=0.005, 0.05, 0.1$の3つの場合の結果を示す.

まずは, Linear構造についての結果を述べる.
\figref{fig:result1:1:linear}より, Linear構造では, 時定数が大きいほど理想的な内部状態とのMSEが小さい値を示した.
また同じ時定数の場合, 提案手法のほうが, 通常のSNNと比較して, MSEが小さい結果となった.
さらに, 提案手法はタイムスケール$a$が$1.0$よりも大きい場合 (入力が低速倍率の場合) において, 理想的な内部状態との差がより小さくなる特徴を示した.

次にCNN系の構造についての結果を述べる.
\figref{fig:result1:1:cnn}, \figref{fig:result1:1:cnn:dropout}, \figref{fig:result1:1:resnet}より, CNN構造, CNN + Dropout構造, ResNet構造では, 提案手法を用いることで, 通常のSNNと比較して, MSEが小さい値を示した.
時定数の初期値とMSEの関係については, Linear構造とは対称的に, その関係性が小さい結果となった.
また, タイムスケールとMSEの関係については, Linear構造と同様に, タイムスケールが大きい場合における提案手法の有効性が高い結果となった.
一方で, \figref{fig:result1:1:cnn:batchnormalization}より, CNN + BatchNormalization構造は提案手法の各タイムスケール$a$におけるMSEが通常のSNNのMSEよりも大きい値を示した.
さらに, タイムスケール$a$が$1.0$よりも大きい値になるに従って, 提案手法のMSEが大きな値を示す結果となった.
\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_linear_mse}
    \caption{Linear構造における時間スケールの違いによる内部状態の変化}
    \label{fig:result1:1:linear}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_csnn_mse}
    \caption{CNN構造における時間スケールの違いによる内部状態の変化}
    \label{fig:result1:1:cnn}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_dropout_mse}
    \caption{CNN + Dropout構造における時間スケールの違いによる内部状態の変化}
    \label{fig:result1:1:cnn:dropout}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_batchnrm_mse}
    \caption{CNN + BatchNormalization構造における時間スケールの違いによる内部状態の変化}
    \label{fig:result1:1:cnn:batchnormalization}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_resnet_mse}
    \caption{ResNet構造における時間スケールの違いによる内部状態の変化}
    \label{fig:result1:1:resnet}
\end{figure}
\clearpage


\subsection{入力速度変化に対する内部状態変化の定性評価}
各ネットワーク構造におけるタイムスケール$a=3.0$の場合の内部状態変化を\figref{fig:result1:2:linear} - \figref{fig:result1:2:resnet}に示す.
本手法は, 入力スパイクのタイムスケーリングに対して, 内部状態も同様にタイムスケーリングに近似することを目指した手法である.
そのため, 定性的には, タイムスケール$a=3.0$の入力スパイク$o_{a=3.0}(t)$が入力されたときの内部状態$v_{a=3.0}(t)$ (グラフ最下段) が, タイムスケール$a=1.0$の入力スパイク$o_{base}(t)$が入力されたときの内部状態を線形補間した$v_{base}(at)$ (グラフ3段目) に近似できていることが望ましい.

まずは, Linear構造についての結果を述べる.
上段2つのグラフは, タイムスケール$a=1.0$の場合の入力スパイク$o_{base}(t)$と, SNNの内部状態$v_{base}(t)$を表す.
上から3段目, 4段目のグラフはそれぞれ, $o_{base}(t), v_{base}(t)$を線形補間によって時間方向にタイムスケーリングした場合の内部状態$o_{base}(at)$と, 提案手法を用いたSNNの内部状態$v_{base}(at)$を表す.
最下段のグラフは, 3段目のタイムスケーリングされたスパイク$o_{base}(at)$が入力されたときの, 通常のSNNの内部状態$v_a^{SNN}(t)$と, 提案手法を用いたSNNの内部状態$v_a^{Proposed}(t)$を表す.
ここで, 青色の破線が通常のSNN, 黄緑色の実線が提案手法を用いたSNNを表す.
4段目の理想的なタイムスケーリングされた内部状態と, 5段目の実際のSNNの内部状態を比較すると, 提案手法を用いたSNNの内部状態は, 理想的な内部状態に近似できていることがわかる.
特に, $timestep=150, 300$あたりの内部状態における活性度の変化が顕著になっている.
通常のSNNでは内部状態がパルス状に活性化していないが, 提案手法を用いたSNNではパルス状に活性化している.
これは, 理想的な内部状態と同様のタイミングでの活性化であり, 提案手法による内部状態のタイムスケーリングが可能であることを示唆している.

次に, CNN系の構造についての結果を述べる.
CNN系の構造では, その入力が2次元であり, 2x2の入力サイズを持つ.
そのため, グラフはヒートマップ上に表記しており, 横軸が時間, 縦軸が入力次元を表す.
また, グラフにおける色は, その値の大きさを表す.
まず, 上段2つのグラフは, Linear構造と同様に, タイムスケール$a=1.0$の場合の入力スパイク$o_{base}(t)$と, SNNの内部状態$v_{base}(t)$を表す.
また, 上から3段目, 4段目のグラフはそれぞれ, $o_{base}(t), v_{base}(t)$を線形補間によって時間方向にタイムスケーリングした場合の内部状態$o_{base}(at)$と, 提案手法を用いたSNNの内部状態$v_{base}(at)$を表す.
さらに, 上から5段目, 6段目のグラフはそれぞれ, 3段目のタイムスケーリングされたスパイク$o_{base}(at)$が入力されたときの, 通常のSNNの内部状態$v_a^{SNN}(t)$と, 提案手法を用いたSNNの内部状態$v_a^{Proposed}(t)$を表す.
\figref{fig:result1:2:cnn}, \figref{fig:result1:2:cnn:dropout}, \figref{fig:result1:2:resnet}より, CNN構造, CNN + Dropout構造, ResNet構造では, 提案手法の内部状態のほうが, 通常のSNNと比較して, 理想的な内部状態のヒートマップに近いことがわかる.
これは, これらの構造において, 提案手法を用いることで, 理想的な内部状態に近似可能であることを示唆している.
一方で, \figref{fig:result1:2:cnn:batchnormalization}より, CNN + BatchNormalization構造では, 提案手法の内部状態が理想的な内部状態のヒートマップから大きくずれていることがわかる.
特に, $timestep$が100から300の区間において, その違いが顕著に見られる.
理想的な内部状態$v_{base}(at)$では, その区間において, 内部状態の値が小さい時間が長い.
一方で, 提案手法の内部状態$v_{a=3.0}^{Proposed}(t)$では, その区間において, 内部状態が断続的に大きな値を示していることがわかる.
このことから, CNN + BatchNormalization構造では, 提案手法を用いることによる, 内部状態のタイムスケーリングが困難であることが示唆された.
\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_linear_volt}
    \caption{Linear構造}
    \label{fig:result1:2:linear}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_csnn_volt}
    \caption{CNN構造}
    \label{fig:result1:2:cnn}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_dropout_volt}
    \caption{CNN + Dropout構造}
    \label{fig:result1:2:cnn:dropout}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_batchnrm_volt}
    \caption{CNN + BatchNormalization構造}
    \label{fig:result1:2:cnn:batchnormalization}
\end{figure}

\begin{figure}[htb]
    \centering
    \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/exp1_resnet_volt}
    \caption{ResNet構造}
    \label{fig:result1:2:resnet}
\end{figure}

\clearpage

\section{考察}
\subsection{提案手法が適用可能なネットワーク構造}
第\ref{sec:result1}節において, 様々なネットワーク構造を持つSNNに提案手法を適用し, 入力速度変化に対する内部状態の挙動を検証した.
その結果, Linear, CNN, CNN+Dropout, ResNet構造では提案手法を用いることで, 入力スパイクのタイムスケーリングに応じて, 内部状態をタイムスケーリングに近似可能である結果となった.
これは, SNNの1つのニューロンについて導出した内部状態タイムスケーリング条件\eqrefc{eq:approximation:condition1:result} - \eqrefc{eq:approximation:condition3:result}が, これらのネットワーク構造において成り立つことを示唆する.
一方で, CNN+BatchNormalization構造では, 提案手法を用いることによる内部状態のタイムスケーリングが困難である結果が得られた.
このような結果は, BatchNormalization層が入力に対するバイアスの付加に対応する操作を持つことが原因であると考えられる.
BatchNormalization層は, $M$個のバッチ入力に対してバッチ内で標準化する操作を持つ.
BatchNormalization層における入力の操作を\eqrefc{eq:batchnormalization:input:mean} - \eqrefc{eq:batchnormalization:input:output}に示す.
\begin{equation}
    \mu_B=\frac{1}{M}\sum_{i=1}^{M}x_i \label{eq:batchnormalization:input:mean}
\end{equation}
\begin{equation}
    \sigma_B^2=\frac{1}{M}\sum_{i=1}^{M}(x_i-\mu_B)^2 \label{eq:batchnormalization:input:variance}
\end{equation}
\begin{equation}
    \hat{x}_i=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}} \label{eq:batchnormalization:input:standardization}
\end{equation}
\begin{equation}
    y_i=\gamma\hat{x}_i+\beta \label{eq:batchnormalization:input:output}
\end{equation}
ここで, $x$はバッチ入力, $i$はバッチ内におけるインデックスを表す.
また, $\mu_B, \sigma_B$はそれぞれバッチ入力の平均, 分散を表す.
BatchNormalization層では, 計算されたこれらの$\mu_B, \sigma_B$を用いて, 入力$x$の標準化を行い, その結果を$\hat{x}$とする.
ここで, $\epsilon$は分散が非常に小さい場合でもバッチ正規化を安定させるためのパラメータである.
その後, $\hat{x}$に対して学習可能なスケーリング係数$\gamma$とバイアス$\beta$を用いて出力$y$を得る.
推論時には, $\mu_B, \sigma_B$は学習中に得られた全バッチの平均と分散が用いられる.

一般的にBatchNormalization層は, Linear層やConv層と活性化関数の間に配置される.
本手法でも同様に, Conv層と提案するLIFモデルの間にBatchNormalization層を使用している.
そのため, BatchNormalization層の出力$\bm{y}$は, LIFモデル (\eqrefc{eq:lif}) における入力シナプス電流$\bm{i}$に相当する (\eqrefc{eq:batchnormalization:lif}).
\begin{align}
    {\tau}\frac{d\bm{v}^l}{dt}&=-\left(\bm{v}^{t-1,l}-v_{rest}\right)+r\bm{i}^{t, l} \notag \\
    &=-\left(\bm{v}^{t-1,l}-v_{rest}\right)+r\bm{y}^{t, l} \notag \\
    &=-\left(\bm{v}^{t-1,l}-v_{rest}\right)+r\left(\gamma \frac{\bm{x}^{t,l}-\mu_B}{\sqrt{\sigma_B^2+\epsilon}} +\beta\right)
    \label{eq:batchnormalization:lif}
\end{align}
これを踏まえて, BatchNormalization層を用いた場合の提案手法におけるSNN内部状態の定式化を\eqrefc{eq:batchnormalization:lif:ideal}, \eqrefc{eq:batchnormalization:lif:actual}に示す.
定式化は第\ref{method:proposed}節で行った手順と同様に, 理想的なSNNの内部状態 (\eqrefc{eq:batchnormalization:lif:ideal}) と実際のSNNの内部状態 (\eqrefc{eq:batchnormalization:lif:actual}) について行う.
\begin{equation}
    V\left(\frac{s}{a}\right) =  \frac{1}{\tau s +1}\left( 
        \frac{a}{s} v_{rest} +rw 
        + \frac{a}{s}r \left(-\gamma \frac{\mu_B}{\sqrt{\sigma_B^2+\epsilon}} +\beta\right)   
    \right)
    \label{eq:batchnormalization:lif:ideal}
\end{equation}
\begin{equation}
    V\left(\frac{s}{a}\right) =  \frac{1}{\tau_{actual} \frac{s}{a} +1}\left( 
        \frac{a}{s} v_{rest} +\frac{1}{a} r_{actual} w
        + \frac{a}{s} r_{actual} \left(-\gamma \frac{\mu_B}{\sqrt{\sigma_B^2+\epsilon}} +\beta\right) 
    \right) 
    \label{eq:batchnormalization:lif:actual}
\end{equation}
これより, BatchNormalization層を用いた場合の, 実際のSNNの内部状態を理想的な内部状態に近似するためには, 以下の3つの条件 (\eqrefc{eq:batchnormalization:lif:approximation:condition1} - \eqrefc{eq:batchnormalization:lif:approximation:condition3}) が成り立つ必要がある.
\begin{equation}
    \frac{1}{\tau_{actual} \frac{s}{a} +1} = \frac{1}{\tau s +1}  
    \label{eq:batchnormalization:lif:approximation:condition1}
\end{equation}
\begin{equation}
    \frac{1}{a} r_{actual} w  = r w  
    \label{eq:batchnormalization:lif:approximation:condition2}
\end{equation}
\begin{equation}
    \frac{a}{s} r_{actual} \left(-\gamma \frac{\mu_B}{\sqrt{\sigma_B^2+\epsilon}} +\beta\right)  = \frac{a}{s} r \left(-\gamma \frac{\mu_B}{\sqrt{\sigma_B^2+\epsilon}} +\beta\right)  
    \label{eq:batchnormalization:lif:approximation:condition3}
\end{equation}
ここで, 提案手法である\eqrefc{eq:approximation:condition1:result} - \eqrefc{eq:approximation:condition3:result}を適用すると, 3つ目の条件\eqrefc{eq:batchnormalization:lif:approximation:condition3}は\eqrefc{eq:batchnormalization:lif:approximation:condition3:result}のように表される.
\begin{equation}
    \frac{a^2}{s} r \left(-\gamma \frac{\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\right)  = \frac{a}{s} r \left(-\gamma \frac{\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\right)  
    \label{eq:batchnormalization:lif:approximation:condition3:result}
\end{equation}
ここで, $a$は入力タイムスケールであり, $a=0$は時間の停止を意味する.
そのため, $a=0$は扱う問題に適さない.
また, $r, \gamma$はそれぞれLIFモデルにおける膜抵抗 (入力重み), BatchNormalization層における出力スケーリング係数 (出力重み) であり, $r=0, \gamma=0$は入出力の値が全て0になることを意味する.
このことから, $a, r, \gamma$は0以外の実数値でなければならない.
そのため, \eqrefc{eq:batchnormalization:lif:approximation:condition3:result}が成り立つためには, バッチ入力の平均値である$\mu_B$が0でなければならない.
しかしながら, ニューラルネットワークの学習では, 前の層のパラメータが変化するに従って, 各層の入力の分布が変化する内部共変量シフトが発生する\cite{batchnorm}.
そのため, 学習データの平均を0に整形した場合であっても, BatchNormalization層における入力の平均値を0にすることは困難である.
従って, \eqrefc{eq:batchnormalization:lif:approximation:condition3:result}が成り立つことは難しいと考えられる.
これらのことから, BatchNormalization層を持つSNNにおいては, 提案手法によるSNN内部状態のタイムスケーリングは困難であると考えられる.

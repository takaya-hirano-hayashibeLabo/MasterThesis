\section{先行研究}
こうしたSNNにおける時間的特性の課題に対して, 先行研究では時定数を学習可能とし, その多様性を実現することが多く行われている\cite{dhsnn, yin2020effective, paramsnn}.
本節では, ニューラルネットワークの重みとバイアスと同様に, 時定数を学習可能とした手法について述べる.
さらに, 1つのニューロンに対して複数の時定数を割り当てることで, SNNの時間的特性を向上させた手法について記述する.

\subsection{Parametric-SNN}
前述の通り, LIFモデルで構成されたSNNの学習アルゴリズムでは, 膜電位の時定数はハイパーパラメータとして扱われる.
そのため, SNNの学習ではニューロン間の接続重みとバイアスのみが最適化される.
しかし, 時系列入力に対するSNN内のニューロンダイナミクスは, その時定数による影響を無視できない.
ある単一のニューロンにおける入力と膜電位の関係の模式図を\figref{fig:lif}に示す.
ニューロンは膜電位$V$を持ち, その値は前方のニューロンからの入力$I$によって変化する.
さらに, その変化量は前方のニューロンの接続重み$w$とそのニューロンの時定数$\tau$に依存する.
また, ニューロンに定常入力が加わったときの膜電位の時間変化を\figref{fig:liffigure}に示す.
ここで, 曲線origin(青線)は時定数$\tau$と重み$w$が基準値のときの膜電位の時間変化を表す.
一方で, $w^+, \tau^+$はそれぞれ重みと時定数が基準値より大きい場合の膜電位の時間変化を表す.
また, $w^-, \tau^-$はそれぞれ重みと時定数が基準値より小さい場合の膜電位の時間変化を表す.
\figref{fig:liffigure}より, ニューロンの接続重み$w$だけでなく, 時定数$\tau$の値によっても膜電位の収束速度が変化することがわかる.
このことから, 時定数を学習可能とすることで, SNNの時間表現能力の向上が期待される.
\begin{figure}[htbp]
    \centering

    \parbox{1.0\textwidth}{
        \centering

        \begin{minipage}{0.243\textwidth}
            \includegraphics[width=1.0\textwidth]{Static/chap1_paramsnn_neuronmodel.pdf}
            \subcaption{LIFモデル}
            \label{fig:lif}
        \end{minipage}
        \hspace{0.02\textwidth}
        \begin{minipage}{0.657\textwidth}
            \includegraphics[width=1.0\textwidth]{Static/chap1_paramsnn_volttrj.pdf}
            \subcaption{LIFモデルにおける膜電位変化}
            \label{fig:liffigure}
        \end{minipage}

        \caption[LIFモデルの模式図と膜電位の時間変化]{
            \cite{paramsnn}
            (a) LIFニューロンモデルの模式図. $I, V$はそれぞれニューロンへの入力と膜電位. 
            $w, \tau$はそれぞれニューロン間の接続重みと時定数. 
            (b) ニューロンに定常入力が加わったときの膜電位の時間変化. 
            $+, -$はそれぞれのパラメータの大小を表す.
        }
    }
\end{figure}

Fangらは, このようなSNNの時定数を学習可能としたアルゴリズムとしてParametric-SNNを提案した\cite{paramsnn}.
Parametric-SNNでは, ニューロンの重み$w$と同様に時定数$\tau$が学習中に最適化される.
そのため, 事前に時定数をハイパーパラメータとして設定する必要がない.
また, SNNの同じ層ではニューロン間の時定数が共有される.
これは, 脳における隣接するニューロンは類似の時間特性を持つ特徴を反映しており, 生物的妥当性が高い.
一方, 異なる層間では時定数は共有されず, 異なる値を持つ.
これによって, SNNが多様な時間的特性の学習が可能となる.
結果として, Parametric-SNNは従来のLIFモデルを用いたSNNよりも画像認識, 動画認識タスクにおいて高いパフォーマンスを発揮した (\tabref{tab:paramsnn:result1}).
さらに, その学習の収束速度も向上することが示された (\figref{fig:paramsnn:traincurve}).
\begin{table}[htbp]
    \centering
    \caption[Parametric-SNNのパフォーマンス]{
        Parametric-SNNのパフォーマンス\cite{paramsnn}.
        タスクは画像認識, 動画認識のベンチマークテスト. 
        $\tau$は学習前の時定数の初期値を表す.
    }
    \label{tab:paramsnn:result1}
    {\small %12ptだとはみ出るので小さく
    \begin{tabular}{ccccc}
        \hline
        Model & Fashion-MNIST & CIFAR-10 & CIFAR10-DVS & DVS128 Gesture\\
        \hline
        Parametric-SNN ($\tau_0=2$) & \textbf{94.38 \%} & \textbf{93.50 \%} & \textbf{74.80 \%} & \textbf{97.57 \%}\\
        SNN ($\tau=2$) & 94.17 \% & 93.03 \% & 73.60 \% & 96.88 \%\\
        \hline
        Parametric-SNN ($\tau_0=16$) & \textbf{94.65 \%} & \textbf{93.23 \%} & \textbf{70.50 \%} & \textbf{92.01 \%}\\
        SNN ($\tau=16$) & 94.47 \% & 47.50\% & 62.40 \% & 76.74 \%\\
        \hline
    \end{tabular}
    }
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Static/chap1_paramsnn_traincurve.pdf}
    \caption[Parametric-SNNとSNNの学習曲線]{
        Parametric-SNNとSNNの学習曲線\cite{paramsnn}.
    }
    \label{fig:paramsnn:traincurve}
\end{figure}


\subsection{DH-SNN}

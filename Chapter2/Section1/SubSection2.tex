\subsection{Spatio-Temporal Back Propagation (STBP) によるSpiking Neural Networkの学習}

一般的に, ANNの学習には誤差逆伝播法が用いられる.
誤差逆伝播法とは, ニューラルネットワークの出力と学習データの損失の勾配を用いて, ネットワークの各パラメータを更新する手法である.
本研究では, 誤差逆伝播法をSNNの学習に拡張したSpatio-Temporal Backpropagation (STBP)則\cite{stbp}を用いる.

SNNは時間的に連続なスパイク列を処理するため, 時間的・空間的な情報を同時に扱う.
さらに, スパイクは0か1の不連続な値を持つため, その勾配は消失または発散してしまう.
このため, ANNに用いられる誤差逆伝播法はそのまま用いることができない.
そこで, STBP則では空間情報と時間情報の両方の損失に対して勾配を計算し, さらに, スパイクの勾配を近似関数を用いて表現することで, 近似的に誤差逆伝播法を実現する.

LIFモデルをニューロンモデルとしたSNNに対し, 初期条件$v(t)|_{t=t_{i-1}}=v(t_{i-1})$としたときの\eqrefc{eq:lif}の線形微分方程式の解は\eqrefc{eq:snn:lif:diff}となる.
\begin{equation}
    v(t) = v \left(t_{i-1}\right) \mathrm{e}^{-\frac{t-t_{i-1}}{\tau}} + i(t)
    \label{eq:snn:lif:diff}
\end{equation}
ここで, 簡易のために$r=1$としているが, その一般性は失われない.

ある時刻$t$のニューロン内部状態$v(t)$の値は\eqrefc{eq:snn:lif:diff}により, 入力シナプス電流$i(t)$の空間的な蓄積と, 以前の内部状態$v(t_{i-1})$の記憶量によって決定される.
そこで, \eqrefc{eq:snn:lif:diff}で得られた解を用いて, ニューロンの内部状態とその入出力は以下のように記述できる(\eqrefc{eq:snn:lif:state},\eqrefc{eq:snn:lif:state2},\eqrefc{eq:snn:lif:state3}).

\begin{align}
    x_i^{t+1,l} &= \sum_{j=1}^{N(l-1)} w_{ij}^{l} o_j^{t+1,l-1} \label{eq:snn:lif:state} \\
    v_i^{t+1,l} &= v_i^{t,l} f\left(o_i^{t,l}\right) + x_i^{t+1,l} + b_i^l \label{eq:snn:lif:state2} \\
    o_i^{t+1,l} &= g\left(v_i^{t+1,l}\right) \label{eq:snn:lif:state3}\\
    where\\
    f\left(x\right) &= \tau \mathrm{e}^{-\frac{x}{\tau}} \label{eq:snn:lif:state4}
\end{align}


ここで, $w_ij$は第$l$層の$i$番目のニューロンと第$l-1$層の$j$番目のニューロンの間のシナプス結合の重み, $b_i^l$は第$l$層の$i$番目のニューロンのバイアスである.
また, $o_j$は0または1の値を持ち, ニューロンの発火スパイクを表す.
$N(l)$は第$l$層のニューロン数に対応する.
\eqrefc{eq:snn:lif:state2}はニューロン内部状態の時間変化を表しており, 1項目の$v_i^{t,l} f\left(o_i^{t,l}\right)$は過去の内部状態の記憶, 2項目と3項目の$x_i^{t+1,l}$と$b_i^l$は現在の入力を表す.

STBPにおける時間的・空間的な損失勾配の概要を\figref{fig:snn:stbp:single}と\figref{fig:snn:stbp:network}に示す. 
\figref{fig:snn:stbp:single}は単一のニューロン, \figref{fig:snn:stbp:network}はネットワークにおける勾配の伝播を表す.
STBPは空間的な勾配と時間的な勾配を伝播することで, ネットワークのパラメータを更新する.
空間的な勾配は, 通常のニューラルネットワークにおける誤差逆伝播法と同様に, 出力の損失勾配から重みとバイアスの勾配を計算する.
一方, 時間的な勾配は, Recurrent Neural Network (RNN) における誤差逆伝播法である Backpropagation Through Time (BPTT) と同様に, 時間的な損失勾配からニューロンの内部状態の勾配を計算する.
\begin{figure}[htb]
    \centering

    \begin{minipage}{0.478\textwidth}
        \centering
        \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/chap2_sec1_stbp1}
        \subcaption{単一ニューロンの勾配}
        \label{fig:snn:stbp:single}
    \end{minipage}
    \hspace{0.02\textwidth}
    \begin{minipage}{0.421\textwidth}
        \centering
        \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/chap2_sec1_stbp2}
        \subcaption{ネットワークの勾配}
        \label{fig:snn:stbp:network}
    \end{minipage}

    \caption[STBP則における時間的・空間的な損失勾配の概要]{
        \cite{stbp}
        (a) 単一ニューロンの勾配.
        $SD, TD$はそれぞれ空間的, 時間的な損失勾配を表す.
        (b) ネットワークの勾配.
    }
\end{figure}


STBP則における損失関数$L$を\eqrefc{eq:snn:stbp:loss}に示す.
\begin{equation}
    L = \frac{1}{2S} \sum_{s=1}^{S} \left(\bm{y}_s - \frac{1}{T}\sum_{t=1}^{T} \bm{o}_s^{t,L}\right)^2 \label{eq:snn:stbp:loss}
\end{equation}
ここで, $\bm{y}_s, \bm{o}_s^{t,l}$はそれぞれ$s$番目のサンプルにおける教師データとSNNの出力を表す.
また, $\frac{1}{T}\sum_{t=1}^{T} \bm{o}_s^{t,L}$は, 単位時間当たりのスパイクの密度を表す.
STBP則では, 教師データとSNNの単位時間当たりのスパイク密度の損失が小さくなるように, ニューラルネットワークの重みとバイアスを更新する.


STBP則におけるニューラルネットワークの重み$\bm{W}$とバイアス$b$損失勾配は以下のように表される.
\begin{align}
    \frac{\partial L}{\partial \bm{b}^l}
    = \sum_{t=1}^{T} \frac{\partial L}{\partial \bm{v}^{t,l}} \frac{\partial \bm{v}^{t,l}}{\partial \bm{b}^l}
    = \sum_{t=1}^{T} \frac{\partial L}{\partial \bm{v}^{t,l}} \label{eq:stbp:b}
\end{align}

\begin{align}
    \frac{\partial L}{\partial \bm{W}^{l}}
    &= \sum_{t=1}^{T} \frac{\partial L}{\partial \bm{v}^{t,l}} \frac{\partial \bm{v}^{t,l}}{\partial \bm{W}^{l}} \notag \\
    &= \sum_{t=1}^{T} \frac{\partial L}{\partial \bm{v}^{t,l}}  \frac{\partial \bm{v}^{t,l}} {\partial \bm{x}^{t,l}} \frac{\partial \bm{x}^{t,l}}{\partial \bm{W}^{l}} 
    = \sum_{t=1}^{T} \frac{\partial L}{\partial \bm{v}^{t,l}} \bm{o}^{t,l-1 \top} \label{eq:stbp:w}
\end{align}
ここで, $\bm{o}^{t,l-1 \top}$は$t$時刻における第$l-1$層の出力の転置ベクトルを表す.
\eqrefc{eq:stbp:b}と\eqrefc{eq:stbp:w}より, ニューラルネットワークの重み$\bm{W}$とバイアス$b$の勾配は, 損失関数$L$に対する内部状態$\bm{v}$の勾配$\partial L / \partial \bm{v}^{t,l}$によって計算できる.


次に$\partial L / \partial \bm{v}^{t,l}$を求めるが, これは以下の4つのパターンによって場合分けして計算することができる.
ここで, $t$は時刻でありその範囲は$t \in \left[1, T\right]$, $l$はニューラルネットワークの層数でありその範囲は$l \in \left[1, L\right]$と定義される.
また, 表記の簡易化のために損失関数$L$のスパイク$o_i^{t,l}$に対する勾配を$\delta_i^{t,l}$と表す (\eqrefc{eq:stbp:delta}).
\begin{equation}
    \delta_i^{t,l} = \frac{\partial L}{\partial o_i^{t,l}} \label{eq:stbp:delta}
\end{equation}

$t=T, l=L$の場合
\begin{equation}
    \frac{\partial L}{\partial o_i^{T,L}}  = - \frac{1}{TS} (y_i -\frac{1}{T}\sum_{k=1}^{T} o_i^{k,L})
\end{equation}

\begin{equation}
    \frac{\partial L}{\partial v_i^{T,L}} 
    = \frac{\partial L}{\partial o_i^{T,L}} \frac{\partial o_i^{T,L}}{\partial v_i^{T,L}} 
    = \delta_i^{T,L} \frac{\partial o_i^{T,L}}{\partial v_i^{T,L}} \label{eq:stbp:v:case1}
\end{equation}

$t=T, l<L$の場合
\begin{equation}
    \frac{\partial L}{\partial o_i^{T,l}} 
    = \sum_{j=1}^{N(l+1)} \delta_j^{T,l+1} \frac{\partial o_j^{T,l+1}}{\partial o_i^{T,l}} 
    = \sum_{j=1}^{N(l+1)} \delta_j^{T,l+1} \frac{\partial g}{\partial v_i^{T, l}} w_{ji}
\end{equation}

\begin{equation}
    \frac{\partial L}{\partial v_i^{T,l}} 
    = \frac{\partial L}{\partial o_i^{T,l}} \frac{\partial o_i^{T,l}}{\partial v_i^{T,l}} 
    = \delta_i^{T,l} \frac{\partial g}{\partial v_i^{T,l}} \label{eq:stbp:v:case2}
\end{equation}

$t<T, l=L$の場合
\begin{align}
    \frac{\partial L}{\partial o_i^{t,L}} 
    &= \delta_i^{t+1,L} \frac{\partial o_j^{t+1,L}}{\partial o_i^{t,L}} + \frac {\partial L} {\partial o_i^{t,L}} \notag \\ 
    &= \delta_i^{t+1,L} \frac{\partial g}{\partial v_i^{t+1,L}} v_i^{t,L} \frac{\partial f}{\partial o_j^{t,L}} + \frac {\partial L} {\partial o_i^{t,L}}
\end{align}

\begin{align}
    \frac{\partial L}{\partial v_i^{t,L}} 
    = \frac {\partial L} {\partial v_i^{t+1,L}} \frac {\partial v_i^{t+1,L}} {\partial v_i^{t,L}} 
    = \delta_i^{t+1,L} \frac {\partial g} {\partial v_i^{t+1,L}} f(o_i^{t,L}) \label{eq:stbp:v:case3}
\end{align}

$t<T, l<L$の場合
\begin{align}
    \frac{\partial L}{\partial o_i^{t,l}} 
    &= \sum_{j=1}^{N(l+1)} \delta_j^{t,l+1} \frac{\partial o_j^{t,l+1}}{\partial o_i^{t,l}} + \frac {\partial L} {\partial o_i^{t+1,l}} \frac{\partial o_i^{t+1,l}}{\partial o_i^{t,l}}  \notag \\
    &= \sum_{j=1}^{N(l+1)} \delta_j^{t,l+1} \frac{\partial g}{\partial v_i^{t,l}} w_{ji} 
    + \delta_i^{t+1,l} \frac{\partial g}{\partial v_i^{t,l}} v_i^{t,l} \frac{\partial f}{\partial o_i^{t,l}}
\end{align}         

\begin{align}
    \frac{\partial L}{\partial v_i^{t,l}} 
    &= \frac {\partial L} {\partial o_i^{t,l}} \frac {\partial o_i^{t,l}} {\partial v_i^{t,l}} + \frac {\partial L} {\partial o_i^{t+1,l}} \frac {\partial o_i^{t+1,l}} {\partial v_i^{t,l}} \notag \\
    &= \delta_i^{t,l} \frac {\partial g} {\partial v_i^{t,l}} 
    + \delta_i^{t+1,l} \frac {\partial g} {\partial v_i^{t+1,l}} f(o_i^{t,l}) \label{eq:stbp:v:case4}
\end{align}


ここで, $g(v)$は0か1のスパイクを出力する関数であり, その勾配$\partial g / \partial v$は消失または爆発してしまう.
そのため, 上記の4つ場合における$\partial L / \partial \bm{v}^{t,l}$の値をそのまま用いて, 重みとバイアスの勾配を計算してもネットワークの学習は進行しない.
そこで, $\partial g / \partial v$を近似関数で表すことでネットワークの学習を進行させる.
近似関数を\eqrefc{eq:stbp:g1} - \eqrefc{eq:stbp:g4}に示す.
\begin{equation}
    \frac{\partial g}{\partial v} = \frac{1}{a_1} \, \text{sign}\left(|v - v_{th}| < \frac{a_1}{2}\right) \label{eq:stbp:g1}
\end{equation}

\begin{equation}
    \frac{\partial g}{\partial v} = \left(\frac{\sqrt{a_2}}{2} - \frac{a_2}{4} |v - v_{th}|\right) \text{sign}\left(\frac{2}{\sqrt{a_2}} - |v - v_{th}|\right) \label{eq:stbp:g2}
\end{equation}

\begin{equation}
    \frac{\partial g}{\partial v} = \frac{1}{a_3} \frac{ \mathrm{e}^{\frac{v_{th}-v}{a_3} }} {\left( 
        1 + \mathrm{e}^{\frac{v_{th}-v}{a_3}}
     \right)^2} \label{eq:stbp:g3}
\end{equation}

\begin{equation}
    \frac{\partial g}{\partial v} = \frac{1}{\sqrt{2\pi a_4}} e^{-\frac{\left(v - v_{th}\right)^2}{2a_4}} \label{eq:stbp:g4}
\end{equation}

ここで, sign関数は入力の符号に応じて, $1, -1, 0$のいずれかを出力する関数である(\eqrefc{eq:stbp:sign}).
\begin{equation}
    \text{sign}(x) = \begin{cases}
        1 & (x > 0) \\
        -1 & (x < 0) \\
        0 & (x = 0)
    \end{cases} \label{eq:stbp:sign}
\end{equation}
また, $a_i$は各関数の積分が1になるような定数である.
それぞれの導関数の外形図を\figref{fig:snn:stbp:pesudo:grad}に示す.
導関数は釣鐘状の形状をしており, ディラックのデルタ関数を近似している.
これらの近似関数はどの形状であっても, その学習精度は大きく変動しないことが知られている.
本研究では, \eqrefc{eq:stbp:g3}のfast sigmoid関数を用いることでSTBP則での学習を行った.

以上よりSTBP則では, 内部状態の勾配\eqrefc{eq:stbp:v:case1} - \eqrefc{eq:stbp:v:case4}と導関数の近似を用いることで, 時空間方向に対して重み$\bm{W}$とバイアス$\bm{b}$を\eqrefc{eq:stbp:b}と\eqrefc{eq:stbp:w}によって学習することが可能となる.

\begin{figure}[htb]
    \centering

    \begin{minipage}{0.333\textwidth}
        \centering
        \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/chap2_sec1_pesudo1}
        \subcaption{スパイク出力関数$g(v)$とその勾配}
        \label{fig:snn:stbp:g}
    \end{minipage}
    \hspace{0.02\textwidth}
    \begin{minipage}{0.370\textwidth}
        \centering
        \includesvg[width=1.0\textwidth, inkscapelatex=false]{Static/chap2_sec1_pesudo2}
        \subcaption{${dg}/{dv}$の近似関数}
        \label{fig:snn:stbp:pesudo:grad}
    \end{minipage}

    \caption[スパイク出力関数の勾配の近似関数]{
        \cite{stbp}
        (a) スパイク出力関数$g(v)$とその勾配. 
        勾配${dg}/{dv}$はディラックのデルタ関数$\delta(v)$で表現される.
        (b) ${dg}/{dv}$の近似関数.
        $h_1, h_2, h_3, h_4$はそれぞれ\eqrefc{eq:stbp:g1} - \eqrefc{eq:stbp:g4}の近似関数を表す.
    }
\end{figure}
